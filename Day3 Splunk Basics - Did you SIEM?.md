# Splunk Basics - Did you SIEM?
Learn how to ingest and parse custom log data using Splunk.

## Learning Objectives
- Ingest and interpret custom log data in Splunk
- Create and apply custom field extractions
- Use Search Processing Language (SPL) to filter and refine search results
- Conduct an investigation within Splunk to uncover key insights

---

# Splunk

Splunk is a platform for collecting, storing, and analysing machine data. It provides various tools for analysing data, including search, correlation, and visualisation. It is a powerful tool that organisations of all sizes can use to improve their IT operations and security posture.

---

# Exploring the Logs

In the Splunk instance, the data has been pre-ingested for us to investigate the incident. On the Splunk interface, click on Search & Reporting on the left panel, as shown below:

<img width="947" height="407" alt="image" src="https://github.com/user-attachments/assets/05c79ad1-fb4a-408d-a2c6-46743bf1008b" />

On the next page, type "index=main" in the search bar to show all ingested logs. Note that we will need to select **All time** as the time frame from the dropdown on the right of the search bar.

<img width="940" height="393" alt="image" src="https://github.com/user-attachments/assets/38f3b7bc-30e3-46cd-b366-04fbc45789fe" />

After running the query, we will be presented with two separate datasets that have been pre-ingested into Splunk. We can verify this by clicking on the "sourcetype" field in the fields list on the left of the page.

<img width="733" height="252" alt="image" src="https://github.com/user-attachments/assets/c319478d-6d82-4dcb-bf00-64698d65c9e2" />

The two datasets are as follows:
- web_traffic: This data source contains events related to web connections to and from the web server.
- firewall_logs: This data source contains the firewall logs, showing the traffic allowed or blocked. The local IP assigned to the web server is 10.10.1.15.
Let's explore the logs and investigate the attack on our servers to identify the culprit.

---

# Initial Triage

Start a basic search across the index using your custom source type web_traffic, using the following query:

**Search query:** index=main sourcetype=web_traffic

Let's break down our result for a better understanding:
- **Search query:** This query retrieves all events from the main index that were tagged with the custom source type web_traffic. This marks the beginning of the investigation.
- **Time range:** The time range is currently set to "All time". In security analysis, this range would be tightened (e.g., to the spike window) after initial data loading.
- **Timeline:** This visual histogram shows the distribution of the 17,172 events over time. The graph indicates the successful daily log volume followed by a distinctive traffic spike (a period of high activity, likely the attack window).
- **Selected fields:** These are the fields currently chosen to be displayed in the summary column of the event list (host, source, sourcetype). They represent basic metadata about the log file itself.
- **Interesting fields:** This pane lists all fields that Splunk has automatically extracted or manually added. Fields prefixed with # (e.g., #date_hour) are automatically generated by Splunk's time commands. The presence of user_agent, path, and client_ip confirms the successful parsing of the web log structure.
- **Event details & field extraction:** This section shows the parsed details of a single event with extracted fields like user_agent, path, status, client_ip, and more.

Now that we have an understanding of the Splunk layout and how to read the logs in Splunk. Let's continue our analysis of the logs.

---

# Visualizing the Logs Timeline

Let's chart the total event count over time, grouped by day, to determine the number of events captured per day. This will help us in identifying the day that received an abnormal number of logs.

**Search query:** index=main sourcetype=web_traffic | timechart span=1d count


